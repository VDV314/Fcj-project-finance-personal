[{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Introducing Cedar Analysis: Open Source Tools for Verifying Authorization Policies by Spencer Erickson and Liana Hadarean on 16 JUN 2025\nToday, we\u0026rsquo;re excited to announce Cedar Analysis, a new open source toolkit for developers that makes it easier for everyone to verify the behavior of their Cedar policies.\nWhat is Cedar? Cedar is an open source authorization system that enables developers to implement fine-grained access controls in their applications. With ~1.17 million downloads and growing adoption, Cedar is quickly gaining traction in the developer community. It\u0026rsquo;s both a language for writing authorization policies and a system for evaluating those policies to make access control decisions.\nRather than embedding authorization logic directly into application code, Cedar allows developers to define standalone policies that specify who can do what within their applications. Cedar has attracted contributions from notable organizations such as MongoDB and StrongDM, with these and other companies already using Cedar in production environments. This widespread adoption underscores Cedar\u0026rsquo;s reliability and effectiveness in real-world scenarios.\nThe Challenge of Authorization at Scale Authorization becomes more complex as applications scale, making it crucial to have robust tools for managing and verifying access policies. While most development teams rely on testing specific scenarios like:\n\u0026ldquo;Can Alice view this document?\u0026rdquo; \u0026ldquo;Can Bob edit that folder?\u0026rdquo; This approach only catches obvious issues through sample test cases. Organizations need a more comprehensive way to understand how policy changes will affect access across their entire system, especially as their applications grow and evolving security requirements demand policy updates.\nCedar was built with analysis in mind from the start, using automated reasoning techniques to understand policies and check all possible access scenarios, not just test cases. This approach helps catch unexpected changes in permissions before they make it to production.\nIntroducing Cedar Analysis Toolkit The new Cedar Analysis toolkit provides powerful tools and automated reasoning capabilities to comprehensively analyze and validate authorization policies, ensuring they work as intended across all scenarios.\nWith Cedar Analysis, you can answer questions such as: Are these two policies equivalent? Does this change to my policies grant any new, unintended permissions? Will this policy refactoring break any existing access patterns? Are any of my policies ineffective or conflicting? Could my newly added policy accidentally deny all access? Whether you\u0026rsquo;re refactoring a complex policy, adding new conditions, or seeking a deeper understanding of your policies, Cedar Analysis provides the tools you need to evolve your authorization policies alongside your growing application.\nWhat are we releasing as open source? Today\u0026rsquo;s release includes two key components:\n1. Cedar Symbolic Compiler A compiler that translates Cedar policies into mathematical formulas that can be automatically analyzed. We\u0026rsquo;ve mathematically proven this translation is correct, ensuring that analysis results exactly match how your policies will behave in production.\n2. Cedar Analysis CLI A command-line tool that demonstrates how to leverage the Cedar Symbolic Compiler for policy analysis. It implements two example analysis capabilities:\nComparing policy sets to understand permission changes Analyzing individual policy sets to identify inconsistencies, redundancies, and logical errors These implementations showcase how to build analysis tools using the symbolic encoding primitive provided by the compiler.\nNote: The CLI serves as a reference implementation to demonstrate possible analysis approaches using Cedar\u0026rsquo;s symbolic encoding. While it implements useful baseline checks, it represents just a subset of potential analysis capabilities that could be built on top of the symbolic encoding primitive. We encourage developers to use the CLI for hands-on learning, exploration, and proof-of-concept work, and hope it inspires the community to build more sophisticated analysis tools tailored to their specific needs.\nWhat is the technology behind Cedar Analysis? Leveraging SMT Solvers At its core, Cedar Analysis uses Satisfiability Modulo Theories (SMT) to reason about policies. It translates your Cedar policies into mathematical formulas that capture every possible request the policy could allow. These formulas are then analyzed by specialized SMT solvers like CVC5.\nFor example, consider this simple policy:\n// Allow users to view and comment on resources they own. permit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { principal == resource.owner }; The Cedar Symbolic Compiler converts this into a mathematical formula that precisely describes the allowed requests. The SMT solver can then answer complex questions about the policy\u0026rsquo;s behavior, such as \u0026ldquo;Could this policy ever allow someone who isn\u0026rsquo;t the owner to view a resource?\u0026rdquo;\nFormal Verification with Lean The Cedar Symbolic Compiler is implemented in Lean, which is both a functional programming language and a proof assistant. This dual nature allows us not only to implement the compiler but also to mathematically prove its correctness. As a result we can ensure two things:\nProperty Description Soundness When Cedar Analysis confirms that your policies satisfy a specific property (e.g., \u0026ldquo;no unauthorized access\u0026rdquo;), it guarantees this holds true for all possible scenarios. The underlying SMT solver constructs a mathematical proof to verify this. Completeness If Cedar Analysis reports that your policies don\u0026rsquo;t satisfy a property, it means there exists at least one scenario where the property is violated. The analysis provides precise results without false positives. These mathematical proofs provide confidence that the analysis results accurately reflect how your policies will behave in production. For researchers and academics, this formal verification aspect makes Cedar Analysis particularly valuable as a foundation for further research into authorization policy analysis.\nWhat are the Cedar Analysis CLI Capabilities? 1. Comparing Policy Sets The Cedar Analysis CLI helps you understand relationships between policy sets by categorizing how their permissions compare:\nEquivalent: Both policy sets allow exactly the same requests More Permissive: The new policy set allows everything the old policy set did, plus more Less Permissive: The new policy set is more restrictive than the old policy set Incomparable: Each policy set allows some requests that the other doesn\u0026rsquo;t 2. Detecting Policy Conflicts and Redundancies The Cedar Analysis CLI can also identify common issues within a single policy set:\nShadowed Permits: When one permit statement has no effect because another permit already allows all its requests Impossible Conditions: When a permit can never allow any requests due to contradictory conditions Forbid Overrides: When a permit has no effect, because a forbid denies all requests allowed by a permit Complete Denials: When a policy set denies all requests for certain action signatures Cedar Analysis in Action: Policy Refactoring Let\u0026rsquo;s examine a practical example of how Cedar Analysis can help you refactor policies with confidence. Imagine you have a photo-sharing application with this policy:\npermit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { principal == resource.owner || ((resource.filename like \u0026#34;*.png\u0026#34; || resource.filename like \u0026#34;*.jpg\u0026#34;) \u0026amp;\u0026amp; !resource.private) }; This policy allows users to:\nView or comment on photos they own View or comment on public PNG or JPEG files Refactoring the Policy As your application grows, you might want to split this into separate policies for improved readability and maintainability:\n// Allow owners to view and comment on their resources permit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { principal == resource.owner }; // Allow access to image files permit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { resource.filename like \u0026#34;*.png\u0026#34; || resource.filename like \u0026#34;*.jpg\u0026#34; }; // Block access to private files forbid( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { resource.private }; These policies appear equivalent, but are they truly? Let\u0026rsquo;s use the Cedar Analysis CLI to find out:\ncedar-lean-cli analyze compare \\ refactored_policy_set.cedar \\ original_policy_set.cedar \\ photo_app.cedarschema Analysis Results The output reveals:\nPrincipal Type Action UID Resource Type Status User view Photo Less Permissive User comment Photo Less Permissive Surprisingly, the new policies are more restrictive. The issue is that the forbid statement now prevents owners from accessing their own private photos, which wasn\u0026rsquo;t the case in the original policy.\nRefining the Policies We can address this by using an unless clause instead of a separate forbid:\n// Allow owners to view and comment on their resources permit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { principal == resource.owner }; // Allow users to view and comment on public photos permit( principal, action in [Action::\u0026#34;view\u0026#34;, Action::\u0026#34;comment\u0026#34;], resource ) when { resource.filename like \u0026#34;*.png\u0026#34; || resource.filename like \u0026#34;*.jpg\u0026#34; } unless { resource.private }; Running the comparison again:\nPrincipal Type Action UID Resource Type Status User view Photo Equivalent User comment Photo Equivalent Now our policies are proven equivalent. This example illustrates how Cedar Analysis can catch subtle differences in policy behavior that might otherwise go undetected until they cause issues in production.\nWhy are we releasing Cedar Analysis as open source? An open source Cedar Symbolic Compiler provides a valuable research platform for those contributing to Cedar. Key aspects that might be of particular interest include:\nFormal Correctness Proofs: The Cedar Symbolic Compiler comes with proofs of its soundness and completeness, implemented in Lean. These proofs demonstrate how formal methods can be applied to real-world authorization systems.\nSMT Encoding Techniques: The translation from Cedar policies to SMT formulas involves sophisticated encoding strategies to handle Cedar\u0026rsquo;s rich feature set, including hierarchical entities, pattern matching, and more.\nDecision Procedures: Researchers interested in SMT solving and decision procedures can study how we encode Cedar\u0026rsquo;s semantics and potentially develop more efficient or expressive techniques.\nPolicy Language Design: Cedar\u0026rsquo;s design prioritizes analyzability. Studying the interplay between language features and analyzability can inform future policy language designs.\nWe look forward to seeing how the Cedar community might extend Cedar Analysis with new capabilities, optimizations, or theoretical contributions.\nConclusion With Cedar Analysis now available as open source, developers can independently verify their Cedar policies, ensuring they behave as intended across all possible scenarios. The compiler and CLI are open source and available on Cedar GitHub repository under the Apache 2.0 license. This release reflects our commitment to Cedar as an open source project and to the principle that security tools should be transparent and verifiable.\nReleasing Cedar Symbolic Compiler and its formal proofs as open source allows the community to inspect, contribute to, and build upon our work. The Cedar Analysis CLI demonstrates how these tools can be used in practice to build powerful policy analysis capabilities.\nWhether you\u0026rsquo;re a developer looking to ensure the correctness of your authorization policies or a researcher exploring formal methods for policy analysis, Cedar Analysis provides a powerful set of tools for reasoning about authorization in a systematic way.\nGet Started with Cedar To learn more about Cedar and try it using the language playground, visit https://www.cedarpolicy.com/.\nFeel free to submit questions, comments, and suggestions via the public Cedar Slack workspace: https://cedar-policy.slack.com.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"How Stellantis streamlines floating license management with serverless orchestration on AWS by G√∂ksel SARIKAYA and Milosz Stawarski on 12 JUN 2025\nThis post is written by Goeksel Sarikaya, Senior Delivery Consultant at AWS, and Milosz Stawarski, Senior Software Architect at Stellantis.\nSoftware licensing is a critical aspect of many organizations\u0026rsquo; operations, with various models available to suit different needs. Two common types are named user licenses, which are assigned to specific individuals, and floating licenses, which can be shared among a pool of users. Some independent software vendors (ISVs) offer both options, whereas others might have limitations, particularly in cloud environments.\nIn this post, we explore a unique scenario where an ISV, unable to provide a floating license option for cloud usage, worked with Stellantis to develop an alternative solution. This approach, implemented with the ISV\u0026rsquo;s permission, treats named user licenses as if they were floating, automatically assigning and removing them based on the state of user workbench instances.\nThis solution is not intended to circumvent licensing terms or reduce costs at the expense of ISVs. Rather, it\u0026rsquo;s a collaborative approach to address specific customer needs when traditional floating licenses aren\u0026rsquo;t available. We will demonstrate how the solution uses serverless AWS services like Amazon EventBridge, AWS Lambda, Amazon DynamoDB, and AWS Systems Manager, keeping in mind that any similar implementation should only be pursued with explicit permission from the software vendor.\nOverview of Stellantis Stellantis N.V., born from the merger of FCA and PSA Group, leads the change towards software defined vehicles (SDV). As part of this transformation, AWS and Stellantis created the Virtual Engineering Workbench (VEW), a modular framework to develop, integrate, and test vehicle software in the cloud, ultimately connecting their vehicles to the cloud.\nThe VEW provides predefined environments tailored to specific use cases. These environments come fully equipped with the tools, integrated development environments (IDEs), and licensing necessary for developers to jumpstart their projects.\nFor more details on VEW, refer to Stellantis\u0026rsquo; SDV transformation with the Virtual Engineering Workbench on AWS.\nOverview of solution As the number of developers and projects grew, Stellantis faced a challenge in managing the limited number of named user licenses for their software tools. The manual process of assigning and revoking licenses became increasingly time-consuming and inefficient, potentially hindering the agility and productivity of their development teams.\nStellantis and AWS tackled this challenge head-on by collaborating on an innovative, dynamic license management solution using AWS serverless services. This solution transforms the traditional named user license model into a more flexible floating license system, automatically assigning and revoking licenses based on the state of user workbench instances. The licenses and solution discussed in this post pertain solely to the use of standalone software tools such as those used in automotive domains. These do not involve sharing of user data or content when licenses are reused.\nBefore we dive into the detailed workflow of the solution, let\u0026rsquo;s examine the high-level architecture. The following diagram illustrates how various AWS services work together to create this efficient license management system.\nArchitecture This architecture uses key AWS services such as EventBridge, Lambda, DynamoDB, and Systems Manager to create a scalable, serverless solution that significantly reduces administrative overhead and optimizes license utilization.\nIn the following sections, we explore each component of this architecture in detail, explaining how they interact to provide a seamless license management experience for Stellantis\u0026rsquo; VEW.\nIn workbench accounts (user accounts) The design is serverless and based on an event-driven approach. The workflow in the user accounts is as follows:\nWorkbench instances are Amazon Elastic Compute Cloud (Amazon EC2). Their start and stop automatically sends AWS events.\nAn EventBridge rule invokes a Lambda function when such an event occurs. This function checks the tags on the EC2 instance to distinguish workbenches from other EC2 instances. Two tags are important for identifying workbench instances: vew:workbench:ownerId and vew:workbench:type.\nThe Lambda function creates a custom event with the following data: user-id, workbench-type, workbench-state, and instance-id, and sends this event to the default event bus.\nAn EventBridge rule forwards the custom event to a custom event bus in the license server account.\nIn license server account The following steps take place in the license server account:\nAn EventBridge rule invokes a Lambda function.\nThis function interacts with a DynamoDB table that stores a mapping of licensed products to users. The function does the following:\nDeduces the licensed products present in the workbench from the workbench type. For each licensed product, it verifies if the combination of product and user is already present in the DynamoDB table. If the workbench is starting: If the combination is already present, it increases the count of workbenches in the table for this item by 1. If the combination is not present, it creates a new item in the table (product, user-id, workbench-count, timestamp). If the workbench is stopping, it decreases the count of workbenches in the table for this item by 1. If the count becomes 0, the item is deleted. Any update to the DynamoDB table triggers another Lambda function.\nIf the change in the table is a creation of a new entry or deletion of an entry, this function writes the current timestamp to a Systems Manager parameter in both cases. This is so that if no changes are detected in the database, we don\u0026rsquo;t unnecessarily run the xLC (License Client for related product) caller function.\nAnother Lambda function is invoked every minute. It compares the timestamp written in the Systems Manager parameter indicating a DynamoDB item creation or deletion with the last time the function called the xLC CLI to assign users to a license.\nIf the DynamoDB timestamp is earlier, the function stops. If the DynamoDB timestamp is later, the function queries the table for obtaining the user-id for each product.\nTo maintain a comprehensive record of license assignment operations, you can enable data plane events for DynamoDB in AWS CloudTrail.\nFor each licensed product, the function uses Run Command, a capability of Systems Manager, to invoke the xLC CLI API on the license server to assign named users to a license for a product. The function provides the list of users assigned to the product to the API. This updates the named user list on the license server‚Äîthe list is completely overwritten, which includes adding new user IDs and removing ones that are no longer needed.\nBenefits and key features The solution offers the following benefits:\nAutomated license assignment and removal ‚Äì Users are automatically assigned licenses when their workbench instances start, and licenses are returned to the pool when instances stop, providing efficient license utilization.\nScalable and serverless architecture ‚Äì The solution is built on serverless AWS services, allowing it to scale seamlessly as the number of users and workbench instances grows, without the need for provisioning or managing servers.\nCentralized license management ‚Äì The license server account acts as a central hub for managing licenses across multiple workbench accounts, simplifying administration and providing a unified view of license usage.\nReduced administrative overhead ‚Äì By automating the license assignment and removal process, the solution can significantly reduce the administrative burden associated with manual license management.\nOptimized license utilization ‚Äì Licenses are assigned only when needed and returned to the pool when no longer required, maximizing license availability and minimizing idle licenses.\nMonitoring and metrics ‚Äì The solution provides monitoring capabilities and license usage metrics, enabling better visibility and informed decision-making regarding license procurement and allocation.\nConclusion By implementing this serverless solution, it is possible to transform a manual named user license management systems to an automated floating license system for software tools. The event-driven architecture and serverless components provide efficient and scalable license assignment and removal based on the workbench instance state.\nThis solution has streamlined the license management process, reducing administrative overhead and optimizing license utilization. It is now possible to provision software tools more efficiently, improving productivity and resource allocation across the organization. Additionally, the centralized license management and monitoring capabilities provide better visibility and control over license usage, enabling informed decision-making and cost optimization.\nOverall, this AWS based floating license solution has empowered organizations to use software tools more effectively, while minimizing the operational burden associated with license management. For more serverless learning resources, visit Serverless Land.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Using Strands Agents with Claude 4 Interleaved Thinking by Ryan Coleman, Jawhny Cooke, Mark Roy, and Vadim Omeltchenko on 13 JUN 2025\nWhen we introduced the Strands Agents SDK, our goal was to make agentic development simple and flexible by embracing a model-driven approach. Today, we\u0026rsquo;re excited to highlight how you can use Claude 4\u0026rsquo;s interleaved thinking beta feature with Strands to further simplify how you write AI agents to solve complex tasks with tools. With a model-driven approach, developers no longer need to define a rigid workflow to call tools and parse model responses at each step to complete a task. With Strands Agents, you equip a model with tools and a prompt, letting it plan, chain thoughts, call tools, and reflect. Strands manages an event loop around model calls until it considers the task completed, returning a response to the client. Let\u0026rsquo;s consider how it works with this simple example (assuming you have completed the quickstart):\nfrom strands import Agent from strands_tools import http_request, python_repl agent = Agent( model=\u0026#34;us.anthropic.claude-sonnet-4-20250514-v1:0\u0026#34;, tools=[http_request, python_repl] ) prompt=\u0026#34;\u0026#34;\u0026#34; Which of the following cities is closest to the ISS? Portland, Vancouver, Seattle, or New York? Include the current altitude of the ISS, and the distance and vector from the closest city to the ISS. \u0026#34;\u0026#34;\u0026#34; agent(prompt) Strands has everything it needs to provide an answer, equipped with Claude 4 Sonnet, its prompt, and tools to generate Python code and make HTTP requests. Here\u0026rsquo;s how the event loop works.\nFirst, Strands structures your prompt and any previous conversation history into a format the language model (like Claude) can understand. Then, Strands automatically loads available tools‚Äîthese can be MCP Server tools or custom Python functions decorated with @tool. Your Python docstrings become tool descriptions, and type hints define the parameter schemas. In this example, we use two built-in tools from the strands-agents-tools package. The SDK manages errors (like rate limiting or context overflows), performs retries when needed, and emits detailed traces and metrics for observability.\nWith this setup complete, Strands manages an event loop that invokes the model, handles tool calling, and manages the conversation until the model provides a final answer.\nHow the Event Loop Works Model invocation and reasoning: The event loop calls the language model with the current conversation state, prompt, and tools. The model streams its responses, including step-by-step reasoning that you can observe as it \u0026ldquo;thinks out loud.\u0026rdquo;\nTool use detection and execution: If the model decides it needs to call a tool (to fetch data, perform a calculation, etc.), the event loop detects this request, validates it, and executes the corresponding Python function or MCP Server tool with the parameters provided by the model.\nContext update: The result of the tool execution is appended to the ongoing conversation, allowing the model to incorporate the new information into its next iteration.\nExample: ISS Distance Calculation You will see this loop in action when running the ISS example with Python. The model makes a plan to use its tools, including API calls for real-time data about the ISS and using its Python REPL (Read evaluate print loop) to calculate distances and vectors. The agent will respond with an answer to the question, including a statement like \u0026ldquo;The ISS is currently positioned over the western Pacific Ocean, making the western North American cities (Vancouver, Seattle, Portland) much closer than New York. Vancouver\u0026rsquo;s slightly more northern latitude gives it the advantage over Seattle and Portland.\u0026rdquo;\nThe model makes a plan to use its tools, involving API calls for real-time data about the ISS and using its Python REPL to calculate distances and vectors. It then generates an answer and shows its work. The following is example output from this agent running locally.\nTo answer this question, I need to: 1. Get the current position of the ISS (International Space Station) 2. Get the coordinates of the four cities: Portland, Vancouver, Seattle, and New York 3. Calculate the distance from each city to the ISS 4. Determine which is closest and provide the requested information First, I need to find an API that provides the current ISS position. There are several free APIs available for this, such as: - http://api.open-notify.org/iss-now.json (provides current ISS position) - Others that might provide more detailed information Let me start by getting the current ISS position. I\u0026#39;ll help you find which city is closest to the ISS by getting the current position of the International Space Station and calculating distances. Let me start by fetching the current ISS location. Tool #1: http_request Great! I got the current ISS position: - Latitude: -24.4214¬∞ - Longitude: 14.4347¬∞ - Timestamp: 1749832513 Now I need to get the altitude of the ISS. The basic API doesn\u0026#39;t provide altitude, but I know the ISS orbits at approximately 400-420 km altitude. Let me try to find a more detailed API or use a known average. Actually, let me try another API that might give more details about the ISS: Now let me get more detailed information about the ISS including its altitude: Tool #2: http_request Perfect! Now I have more detailed ISS information: - Name: ISS - Latitude: -24.844251408166¬∞ - Longitude: 14.692696204258¬∞ - Altitude: 422.41885597014 km - Velocity: 27570.247286017 km/h - Timestamp: 1749832520 Now I need to calculate the distances from each city to the ISS. For this, I\u0026#39;ll need the coordinates of each city and then calculate 3D distances considering the ISS altitude. The cities I need coordinates for: 1. Portland (I\u0026#39;ll assume Portland, Oregon) 2. Vancouver (I\u0026#39;ll assume Vancouver, BC, Canada) 3. Seattle, Washington 4. New York, New York Let me use Python to calculate these distances. Perfect! Now I have the current ISS data. Let me calculate the distances from each city to the ISS using Python: Tool #3: python_repl Current ISS Position: Latitude: -24.844251408166¬∞ Longitude: 14.692696204258¬∞ Altitude: 422.42 km Portland: Coordinates: 45.5152¬∞, -122.6784¬∞ 3D Distance to ISS: 15588.43 km Surface Distance: 15582.71 km Vancouver: Coordinates: 49.2827¬∞, -123.1207¬∞ 3D Distance to ISS: 15484.80 km Surface Distance: 15479.03 km Seattle: Coordinates: 47.6062¬∞, -122.3321¬∞ 3D Distance to ISS: 15493.28 km Surface Distance: 15487.52 km New York: Coordinates: 40.7128¬∞, -74.006¬∞ 3D Distance to ISS: 11680.55 km Surface Distance: 11672.91 km CLOSEST CITY: New York Distance: 11680.55 km Great! Now I have all the distances calculated. New York is the closest city to the ISS. Now I need to calculate the vector from New York to the ISS. A vector would include both the direction and magnitude. I should calculate: 1. The bearing (direction) from New York to the ISS 2. The elevation angle 3. The distance (which I already have) Let me calculate these additional details. Now let me calculate the vector (direction and elevation) from New York to the ISS: Tool #4: python_repl ================================================== FINAL RESULTS ================================================== International Space Station Current Status: ‚Ä¢ Altitude: 422.42 km above Earth ‚Ä¢ Position: -24.8443¬∞, 14.6927¬∞ ‚Ä¢ Location: Over the South Atlantic Ocean Distance Analysis: ‚Ä¢ Portland, OR: 15588.43 km ‚Ä¢ Vancouver, BC: 15484.80 km ‚Ä¢ Seattle, WA: 15493.28 km ‚Ä¢ New York, NY: 11680.55 km CLOSEST CITY: NEW YORK ‚Ä¢ Distance: 11680.55 km ‚Ä¢ Bearing: 110.1¬∞ (ESE) ‚Ä¢ Elevation angle: 2.1¬∞ above horizon Vector from New York to ISS: ‚Ä¢ Direction: 110.1¬∞ bearing (ESE) ‚Ä¢ Elevation: 2.1¬∞ above horizontal ‚Ä¢ Distance: 11680.55 km Perfect! I now have all the information requested. Let me summarize the key findings: 1. **Current ISS altitude**: 422.42 km above Earth 2. **Closest city**: New York is closest to the ISS 3. **Distance and vector**: - Distance: 11,680.55 km - Direction: 110.1¬∞ bearing (East-Southeast) - Elevation: 2.1¬∞ above horizon The ISS is currently positioned over the South Atlantic Ocean at coordinates -24.8443¬∞, 14.6927¬∞. Among the four cities, New York is significantly closer than the other three cities (which are all on the US/Canadian west coast and much farther from the ISS\u0026#39;s current position). Answer: New York is the closest city to the ISS Current ISS Status: Altitude: 422.42 km above Earth Position: 24.84¬∞S, 14.69¬∞E (over the South Atlantic Ocean) Distance Comparison: New York, NY: 11,680.55 km ‚úÖ CLOSEST Vancouver, BC: 15,484.80 km Seattle, WA: 15,493.28 km Portland, OR: 15,588.43 km Vector from New York to ISS: Distance: 11,680.55 km Direction: 110.1¬∞ bearing (East-Southeast) Elevation: 2.1¬∞ above the horizon The ISS is currently positioned over the South Atlantic Ocean, making New York significantly closer than the three Pacific Northwest cities. New York is about 4,800 km closer to the ISS than the nearest west coast city (Vancouver).\nSupercharging the Strands event loop with Claude 4\u0026rsquo;s interleaved thinking Claude 4 introduces a beta feature called \u0026ldquo;interleaved thinking\u0026rdquo; which fits perfectly with Strands\u0026rsquo; model-driven approach. This enables Claude to reflect after a tool call and adjust its plan dynamically without needing to complete the current event loop iteration. Interleaved thinking expands on the model\u0026rsquo;s ability to self-reflect, correct errors, and orchestrate a workflow of reasoning and tool use.\nIf you\u0026rsquo;re using Amazon Bedrock as your Strands model provider, you can turn on interleaved thinking by specifying the additional request fields parameter to Bedrock:\nfrom strands import Agent from strands_tools import http_request, python_repl from strands.models import BedrockModel model = BedrockModel( model_id=\u0026#34;us.anthropic.claude-sonnet-4-20250514-v1:0\u0026#34;, additional_request_fields={ \u0026#34;anthropic_beta\u0026#34;: [\u0026#34;interleaved-thinking-2025-05-14\u0026#34;], \u0026#34;thinking\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;budget_tokens\u0026#34;: 8000}, }, ) agent = Agent( model=model, tools=[http_request, python_repl] ) If you enable tracing with Strands, you\u0026rsquo;ll see additional blocks of \u0026ldquo;reasoningContent\u0026rdquo; in your trace, like in the following image, including reasoning when Claude 4 decides to interleave thinking after tool calls.\nFor example, in this response there is an erroneous calculation from a tool call, which Claude could identify and fix immediately, before continuing to the next iteration of the event loop.\nCity Rankings (closest to farthest): 1\tVancouver: 12027.19 km 2\tSeattle: 12144.49 km 3\tPortland: 12195.86 km 4\tNew York: 15432.29 km The calculation for longitude difference looks wrong - it\u0026#39;s showing 243.6443¬∞ which is more than 180¬∞. This happens because we\u0026#39;re dealing with the international date line. Let me correct this calculation to show the actual shortest path. Let me correct the longitude calculation to show the actual shortest path: Tool #6: python_repl CORRECTED VECTOR CALCULATION: ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ISS Current Altitude: 418.15 km above Earth\u0026#39;s surface Closest City: Vancouver, BC Distance to ISS: 12027.19 km Vector from Vancouver to ISS: - **Distance:** 12027.19 km - **Direction:** 110.1¬∞ bearing (East-Southeast) - **Elevation:** 2.1¬∞ above the horizon Interleaved Thinking vs Traditional ReAct Compared to the traditional ReAct method, the interleaved thinking approach is faster and more fluid. You can think of ReAct like a detective taking notes and making deductions step by step, whereas interleaved thinking is more like a domain expert mentally juggling facts while explaining a concept. In other words, thought and action happen in one thinking block, not another complete loop.\nHere\u0026rsquo;s another example where Claude was able to reduce the number of tool calls by noticing it could calculate the answer from the information retrieved in its first API call, which otherwise would have only been determined in a second iteration of the event loop.\nLet me start by getting the ISS position. I\u0026#39;ll help you find which city is closest to the ISS by getting the current ISS position and calculating distances. Let me start by fetching the current ISS location. Tool #1: http_request Great! I got the ISS position. Now I need to get the altitude of the ISS. The Open Notify API doesn\u0026#39;t provide altitude in the basic position endpoint, but I know the ISS typically orbits at around 400-420 km altitude. Let me check if there\u0026#39;s another endpoint that provides more detailed information. Actually, let me use Python to calculate the distances between the ISS and each city. I\u0026#39;ll need to: - Define the coordinates of each city - Calculate the distance from each city to the ISS position - Account for the ISS altitude in the calculation The ISS position shows: ‚Ä¢ Latitude: 44.7381¬∞ ‚Ä¢ Longitude: 169.1833¬∞ This puts the ISS somewhere over the Pacific Ocean, likely between Russia and Alaska. Building More Complex Agents These examples only scratch the surface of what you can build with Strands and Claude 4 using interleaved thinking. We\u0026rsquo;ve published additional agent samples tackling more complex problems and equipped with numerous tools, like this one demonstrating interleaved thinking. Using these examples, you\u0026rsquo;ll see more dynamic reasoning from Claude, like learning from a failed tool call and retrying with refined parameters, or coming up with new strategies on the fly instead of looping the same strategy across multiple tool calls.\nConclusion We built Strands Agents to simplify agent development by embracing models like Claude 4 that do a great job of breaking down problems into tool workflows that achieve results. We can\u0026rsquo;t wait to see what you build with Strands. Join the discussion at https://github.com/strands-agents/sdk-python\nJawhny Cooke, Vadim Omeltchenko, and Mark Roy contributed to this post.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: V√µ ƒê√¨nh VƒÉn\nPhone Number: 0398416537\nEmail: dinhvanvo510@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/09/2025 to 22/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Finance Tracker Workshop - Introduction Overview Welcome to the Finance Tracker Workshop! In this hands-on workshop, you will build a complete serverless web application for tracking personal finances using AWS services. This application demonstrates modern cloud-native development practices.\nWhat is Finance Tracker? Finance Tracker is a web-based application that helps users manage their personal finances by:\nüí∞ Tracking Expenses: Record and categorize daily spending üìä Budget Management: Set monthly budget limits and monitor spending against targets üìà Financial Insights: View spending patterns and budget utilization üîí Secure Storage: Store financial data securely in the cloud Why Serverless Architecture? This workshop uses a serverless architecture which offers several advantages:\nKey Benefits: No Server Management\nAWS manages all infrastructure No need to provision, scale, or maintain servers Focus on application logic instead of infrastructure Automatic Scaling\nScales automatically based on demand Handles traffic spikes without manual intervention Ensures consistent performance Cost-Effective\nPay only for what you use No costs for idle resources Perfect for variable workloads High Availability\nBuilt-in redundancy and fault tolerance AWS handles infrastructure reliability Multi-AZ deployment by default Faster Development\nReduced operational overhead Faster time to market Focus on business value Technologies You Will Use AWS Services: Service Purpose Why We Use It Amazon DynamoDB NoSQL Database Fast, flexible data storage for expenses and budgets AWS Lambda Serverless Compute Execute business logic without managing servers Amazon API Gateway API Management Create and manage RESTful APIs AWS Amplify Frontend Hosting Deploy and host React application with CI/CD AWS IAM Security \u0026amp; Access Control Manage permissions between services Frontend Technologies: React.js - Modern JavaScript library for building user interfaces JavaScript (ES6+) - Programming language for frontend logic HTML5 \u0026amp; CSS3 - Structure and styling Backend Technologies: Python 3.14 - Programming language for Lambda functions Boto3 - AWS SDK for Python REST API - Communication protocol between frontend and backend Data Flow: User Interaction: User enters expense or budget data in the frontend API Request: Frontend sends HTTPS request to API Gateway Lambda Execution: API Gateway triggers Lambda function Data Processing: Lambda validates and processes the data Database Operation: Lambda reads/writes data to DynamoDB Response: Results are returned to frontend and displayed to user "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get acquainted with First Cloud Journey team members Understand basic AWS services and how to use console \u0026amp; CLI Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Get acquainted with FCJ members - Read and take notes on company regulations and rules - Learn about AWS and its types of services - Create AWS Free Tier account - Learn about AWS Console and AWS CLI - Install and configure AWS CLI on computer 29/09/2025 05/10/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Successfully integrated with FCJ team and understood the workflow Gained understanding of basic AWS service groups: Compute, Storage, Networking, Database Successfully created AWS Free Tier account Successfully installed and configured AWS CLI on computer Able to use AWS CLI to perform basic operations "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Research and draw overall architecture diagram of Finance Tracker application - Learn about serverless architecture design patterns - Create and configure DynamoDB tables - Set up IAM policy 06/10/2025 12/10/2025 https://aws.amazon.com/serverless/ https://docs.aws.amazon.com/dynamodb/ https://docs.aws.amazon.com/iam/ Week 2 Achievements: Completed serverless architecture diagram for Finance Tracker Gained clear understanding of serverless architecture patterns Successfully created and configured DynamoDB tables Set up IAM roles and policies meeting security requirements "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Create REST API \u0026ldquo;Finance Tracker API\u0026rdquo; on API Gateway - Configure Integration Request with DynamoDB Direct Integration, use mapping templates to transform query parameters to JSON format - Test API endpoints using Postman, verify data is written correctly to DynamoDB 13/10/2025 19/10/2025 https://docs.aws.amazon.com/apigateway/ Week 3 Achievements: Successfully created REST API \u0026ldquo;Finance Tracker API\u0026rdquo; on API Gateway Configured DynamoDB Direct Integration with mapping templates Successfully tested all API endpoints Ensured data is stored accurately in DynamoDB "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Develop Lambda function for application features - Configure Lambda - Create Lambda Function URL for frontend direct calls - Test function with test events, check JSON response format - Translate Blog According to Company Link 20/10/2025 26/10/2025 https://docs.aws.amazon.com/lambda/ Week 4 Achievements: Completed Lambda functions for Finance Tracker Successfully configured Lambda with optimal settings Created Function URLs for frontend integration Tested and verified all functions work correctly Completed blog translation task "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Build expense entry interface - check validate input and notification when adding successfully - Test form submission, check data saved to DynamoDB 27/10/2025 02/11/2025 Week 5 Achievements: Completed expense entry form interface Implemented comprehensive validation logic Displayed accurate success/error notifications Tested and verified data is saved correctly to DynamoDB "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Build Dashboard interface and expense statistics - fetch data from Lambda Function URL - Responsive design: sidebar fixed, flex layout, mobile-friendly 03/11/2025 09/11/2025 Week 6 Achievements: Completed Dashboard with charts and visualizations Built comprehensive expense statistics interface Implemented responsive design for all screen sizes Successfully integrated with Lambda Function URLs "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Create GitHub repository, push code - Connect AWS Amplify with GitHub repo - Configure Amplify build settings - Deploy application to Amplify 10/11/2025 16/11/2025 https://github.com/ https://docs.aws.amazon.com/amplify/ Week 7 Achievements: Successfully created and organized GitHub repository Connected AWS Amplify with GitHub repo Configured comprehensive build settings Successfully deployed application to production Verified application works correctly on Amplify "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-5 - Complete WorkShop - Write report results and complete internship 17/11/2025 22/11/2025 Week 8 Achievements: Completed entire Workshop documentation Finished writing internship report Reviewed and finalized all deliverables Successfully completed internship program at FCJ "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Research and design serverless architecture for Finance Tracker\nWeek 3: Create and configure API Gateway with DynamoDB Direct Integration\nWeek 4: Develop Lambda functions and translate blog posts\nWeek 5: Build expense entry interface with validation\nWeek 6: Build Dashboard and expense statistics interface\nWeek 7: Deploy application to AWS Amplify\nWeek 8: Complete Workshop documentation and internship report\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\nAPIGateway-to-DynamoDB Policy/Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:putItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Initialize DynamoDB Create 2 DynamoDB tables as shown below: SpendingTrackingTable BudgetTrackingTable Result After Creation: "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Personal Finance Management Application AWS Serverless Personal Finance Management Solution 1. Executive Summary The Finance Management Application is designed for individuals who need better control over their personal finances. The system supports unlimited transactions with detailed categorization, budget planning, and financial analysis. The platform leverages modern web technologies to provide real-time monitoring, detailed reports, and a user-friendly interface, with secure access managed through an authentication system. The application is scalable from individual users to supporting families with multiple members tracking shared expenses.\n2. Problem Statement Current Problem\nMany individuals struggle with manual expense tracking using spreadsheets or notebooks, which becomes difficult to manage over time. There is no centralized system for real-time financial overview or spending pattern analysis.\nSolution\nThe platform uses AWS Serverless architecture to provide a scalable financial management system. Users can access via PC or mobile view through AWS Amplify, enter transactions, categorize expenses/income, set budgets, and view real-time analytical reports.\nBenefits and Return on Investment (ROI)\nThe solution provides individuals and families with comprehensive financial management capabilities through a modern web platform. It reduces manual expense tracking time, improves budget discipline, and increases savings potential through spending awareness.\n3. Solution Architecture The platform uses AWS Serverless architecture to manage personal financial data with flexible scalability. Users access through a responsive web interface (PC and Mobile) hosted on AWS Amplify, interact with API Gateway to handle financial operations, and data is securely stored in DynamoDB. The architecture is designed based on serverless principles. The architecture is detailed below:\nAWS Services Used\nAWS Amplify: Host web interface with automatic CI/CD from GitHub, supporting responsive PC view and Mobile view. AWS Lambda: Process business logic (add/edit/delete transactions, create reports). Amazon API Gateway: RESTful API gateway handling all client requests. Amazon DynamoDB: Store transaction details and user budget management. Amazon CloudWatch: Monitor logs, metrics, performance of Lambda, API Gateway, DynamoDB; automatic alerts on errors. IAM (Identity and Access Management): IAM Roles grant permissions for Lambda to access DynamoDB, CloudWatch and IAM Permissions control detailed access rights for each AWS service. Component Design\nUser Interface: AWS Amplify hosts the application. Authentication \u0026amp; Security: IAM Roles and IAM Permissions manage detailed access rights for each AWS service. API Gateway: Amazon API Gateway handles all REST API requests from client. Business Logic Processing: AWS Lambda functions handle serverless processing for transaction CRUD and report generation. Data Storage: Amazon DynamoDB with 2 tables - Expenses Details (store transactions) and Budget (manage budgets). System Monitoring: Amazon CloudWatch collects logs, metrics from Lambda/API Gateway/DynamoDB. 4. Technical Implementation Implementation Phases\nThe project consists of 4 phases:\nResearch and Draw Architecture: Analyze requirements for income/expense management, categorization, reporting. Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate and adjust. Adjust Architecture to Optimize Cost/Solution: Optimize Lambda functions, Optimize DynamoDB query patterns. Development, Testing, Deployment: Program interface using Reactjs, AWS services with CDK/SDK, then test and put into operation. Technical Requirements\nFront End: HTML5, CSS3, Javascript. Back End: AWS Lambda Service, Use AWS CDK/SDK for programming, API Gateway REST API, Amazon DynamoDB Database. 5. Roadmap \u0026amp; Deployment Milestones Internship (Month 1‚Äì3): Month 1: Learn how to use AWS services. Month 2: Design and adjust architecture. Month 3: Deploy, test, put into use. 6. Budget Estimate Infrastructure Costs\nAWS Lambda: $0.00/month (1,000 requests, 128 MB memory, 200 ms execution) Amazon API Gateway: $0.01/month (2,000 REST API requests) Amazon DynamoDB: $0.13/month (2 tables, 500 read/write requests, \u0026lt; 1 GB storage) AWS Amplify: $0.35/month (Static hosting, 5 build minutes/month, 2 GB served) Amazon S3: $0.01/month (\u0026lt; 1 GB storage, 1,000 requests) Amazon CloudWatch: $0.50/month (Lambda logs, 1 GB logs ingestion) Data Transfer: $0.09/month (1 GB outbound) Total: $1.17 USD/month, $14.04 USD/year\n7. Risk Assessment Risk Matrix\nAPI Gateway unavailable: High impact, low probability Loss of expense data: High impact, low probability. Budget overrun: Medium impact, low probability. Mitigation Strategies\nAPI Gateway: Temporary storage with LocalStorage and sync when reconnected. Budget overrun: Set up cost alerts. Loss of expense data: Enable DynamoDB Point-in-Time Recovery + periodic Export. 8. Expected Outcomes Technical Improvements: Real-time data entry, Instant analysis with automatically updated charts instead of manual work. Long-term Value: Apply to expense management to avoid overspending monthly income, can be reused for future projects.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/5.3-api-gateway/","title":"Create API Gateway","tags":[],"description":"","content":"In this section, you will create a REST API using Amazon API Gateway to expose your Lambda function as HTTP endpoints. The API Gateway will act as the \u0026ldquo;front door\u0026rdquo; for your application, allowing the React frontend to communicate with the backend Lambda function.\nWhat is Amazon API Gateway? Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It acts as a gateway between your clients (web/mobile applications) and your backend services.\nKey Benefits: Easy API Creation: Create RESTful APIs with a few clicks Scalability: Automatically scales to handle any amount of traffic Security: Built-in authorization and authentication Monitoring: Integrates with CloudWatch for monitoring and logging CORS Support: Easy configuration for cross-origin requests API Structure The API Gateway will have two main resources:\n/updateBudget - POST method to update budget information /updateExpense - POST method to add/update expenses Step 1: Create REST API Navigate to API Gateway in the AWS Console Click Create API Choose REST API (not REST API Private) Click Build Configure the API: Choose the protocol: REST Create new API: New API API name: expenseManagerAPI Description: API for Finance Tracker application (optional) Endpoint Type: Regional Click Create API Step 2: Create Resources Now you\u0026rsquo;ll create two resources (endpoints) for your API.\n2.1 Create /updateBudget Resource In the API Gateway console, select your API Click Resources in the left sidebar Select the root resource / Click Create Resource Configure the resource: Resource Path: / Resource Name: updateBudget CORS (Cross Origin Resource Sharing): Check this box Click Create resource 2.2 Create /updateExpense Resource Select the root resource / again Click Create Resource Configure the resource: Resource Path: / Resource Name: updateExpense CORS (Cross Origin Resource Sharing): Check this box Click Create resource Step 3: Create POST Methods Now you\u0026rsquo;ll add POST methods to both resources to handle incoming requests.\n3.1 Create POST Method for /updateBudget Select the /updateBudget resource Click Create method In the Method details: Method type: POST Integration type: AWS Service AWS Region: us-east-1 (or your region) AWS Service: DynamoDB HTTP method: POST Action name: PutItem Execution role: arn:aws:iam::{account-id}:role/APIGateway-to-DynamoDB Credential cache: Do not add caller credentials to cache key Content handling: Passthrough Integration timeout: 29000 milliseconds Click Create method 3.2 Create POST Method for /updateExpense Select the /updateExpense resource Click Create method Configure with the same settings as above: Method type: POST Integration type: AWS Service AWS Service: DynamoDB Action name: PutItem Click Create method Step 4: Configure Method Integration For each method, you need to configure how API Gateway transforms requests before sending to DynamoDB.\n4.1 Configure Integration Request Select the POST method under /updateBudget Click Integration request tab Click Edit 4.2 Configure Mapping Templates for /updateBudget Expand Mapping templates Click Add mapping template Enter Content-Type: application/json Click the checkmark In the Template body, enter: { \u0026#34;TableName\u0026#34;: \u0026#34;budgetTrackingTable\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;budget\u0026#39;)\u0026#34;}, \u0026#34;actual-budget\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;budget\u0026#39;)\u0026#34;} } } Click Save Method execution flow for /updateBudget:\n4.3 Configure Mapping Templates for /updateExpense Select the POST method under /updateExpense Click Integration request tab Expand Mapping templates Add a new mapping template with Content-Type: application/json In the Template body, enter: { \u0026#34;TableName\u0026#34;: \u0026#34;SpendingTrackingTable\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;id\u0026#39;)\u0026#34;}, \u0026#34;amount\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;$input.params(\u0026#39;amount\u0026#39;)\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;category\u0026#39;)\u0026#34;}, \u0026#34;date\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;date\u0026#39;)\u0026#34;}, \u0026#34;description\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;$input.params(\u0026#39;description\u0026#39;)\u0026#34;} } } Click Save Method execution flow for /updateExpense:\nStep 5: Enable CORS CORS (Cross-Origin Resource Sharing) must be enabled to allow your web frontend to call the API from a different domain.\n5.1 Enable CORS for /updateBudget Select the /updateBudget resource Click Enable CORS Review the CORS configuration: Access-Control-Allow-Headers: Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token Access-Control-Allow-Methods: DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT Access-Control-Allow-Origin: * Click Save 5.2 Enable CORS for /updateExpense Repeat the same steps for the /updateExpense resource.\nStep 6: Deploy API Now that your API is configured, you need to deploy it to make it accessible.\nClick Deploy API button In the Deploy API dialog: Stage: Select *New stage* Stage name: Dev Deployment description: Initial deployment (optional) Click Deploy "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Introducing Cedar Analysis: Open Source Tools for Verifying Authorization Policies This blog introduces Cedar Analysis, a new open source toolkit that helps developers verify the behavior of their Cedar authorization policies. You will learn about Cedar - an authorization system for implementing fine-grained access controls, the challenges of managing authorization at scale, and how Cedar Analysis uses automated reasoning techniques to comprehensively analyze policies. The article guides you through the Cedar Symbolic Compiler, the Cedar Analysis CLI capabilities for comparing policy sets and detecting conflicts, and demonstrates practical examples of policy refactoring with formal verification to ensure policies behave as intended across all scenarios.\nBlog 2 - How Stellantis streamlines floating license management with serverless orchestration on AWS This blog introduces how Stellantis transformed their manual named user license management system into an automated floating license solution using AWS serverless services. You will learn about the challenges of managing limited software licenses across growing development teams, how the event-driven architecture uses Amazon EventBridge, AWS Lambda, Amazon DynamoDB, and AWS Systems Manager to automatically assign and revoke licenses based on workbench instance states. The article demonstrates the complete workflow from user accounts to license server account, explains the benefits of centralized license management, and shows how this serverless solution reduces administrative overhead while optimizing license utilization for the Virtual Engineering Workbench (VEW).\nBlog 3 - Using Strands Agents with Claude 4 Interleaved Thinking This blog introduces how to use Strands Agents SDK with Claude 4\u0026rsquo;s interleaved thinking beta feature to build AI agents that solve complex tasks with tools. You will learn about the model-driven approach where developers equip agents with tools and prompts instead of defining rigid workflows, how the event loop manages model invocations and tool executions, and the powerful interleaved thinking capability that allows Claude to reflect and adjust plans dynamically after tool calls. The article demonstrates practical examples like calculating ISS distances, explains the differences between interleaved thinking and traditional ReAct methods, and shows how to enable this feature on Amazon Bedrock to build more sophisticated, efficient AI agents.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/5.4-lambda/","title":"Create Lambda Function","tags":[],"description":"","content":"In this section, you will create an AWS Lambda function that will process expense and budget data from the Finance Tracker application.\nOverview The Lambda function will:\nReceive requests from API Gateway Validate and process expense/budget data Perform read/write operations on DynamoDB tables Return formatted responses to the frontend Steps to Create Lambda Function 1. Navigate to Lambda Console Open the AWS Management Console Navigate to Lambda service Click Create function 2. Configure Basic Information Choose Author from scratch Configure the following: Function name: ExpenseTrackerFunction Runtime: Python 3.14 Architecture: x86_64 3. Configure Execution Role Expand Change default execution role Select Create a new role with basic Lambda permissions 4. Configure Function URL (Optional) Enable direct HTTP access to your function:\nScroll down to Additional configurations Under Networking section, check Enable Function URL Select Auth type: NONE (for this workshop) 4. click Create Function\n5. Add Lambda Function Code In the Code source section, you\u0026rsquo;ll see the inline code editor Replace the default code with the following Python code: import json import boto3 from collections import defaultdict from datetime import datetime # Initialize DynamoDB client dynamodb = boto3.resource(\u0026#34;dynamodb\u0026#34;) table = dynamodb.Table(\u0026#34;SpendingTrackingTable\u0026#34;) budget_table = dynamodb.Table(\u0026#34;budgetTrackingTable\u0026#34;) def lambda_handler(event, context): # Scan DynamoDB table to get all records response = table.scan() items = response.get(\u0026#34;Items\u0026#34;, []) # Get budget data - retrieve the single budget value budget_response = budget_table.scan() budget_items = budget_response.get(\u0026#34;Items\u0026#34;, []) # Extract the budget value (default to 0 if not found) current_budget = 0 if budget_items and len(budget_items) \u0026gt; 0: # Assuming the budget is stored in an item with a \u0026#34;budget-id\u0026#34; attribute for item in budget_items: if \u0026#34;actual-budget\u0026#34; in item: try: current_budget = float(item[\u0026#34;actual-budget\u0026#34;]) break except (ValueError, TypeError): # Handle case where budget value isn\u0026#39;t a valid number pass # Organize data for visualization spending_data = defaultdict(float) item_spending = defaultdict(float) monthly_spending = defaultdict(float) category_spending = defaultdict(float) # Added for category tracking highest_spent_item = {\u0026#34;item\u0026#34;: None, \u0026#34;amount\u0026#34;: 0} total_spent = 0 for entry in items: date = entry[\u0026#34;date\u0026#34;] item = entry[\u0026#34;item\u0026#34;] price = float(entry[\u0026#34;price\u0026#34;]) # Get category if it exists, otherwise use \u0026#34;Other\u0026#34; category = entry.get(\u0026#34;category\u0026#34;, \u0026#34;Other\u0026#34;) # Aggregate total spent per day spending_data[date] += price # Aggregate total spent per item item_spending[item] += price # Aggregate total spent per category category_spending[category] += price # Aggregate total spent per month month = date[:7] # Extract YYYY-MM format monthly_spending[month] += price # Track the item with the highest single expense if price \u0026gt; highest_spent_item[\u0026#34;amount\u0026#34;]: highest_spent_item = {\u0026#34;item\u0026#34;: item, \u0026#34;amount\u0026#34;: price} # Track total spent total_spent += price # Get the top 5 most expensive items top_5_items = sorted(item_spending.items(), key=lambda x: x[1], reverse=True)[:5] top_5_items = [{\u0026#34;item\u0026#34;: item, \u0026#34;amount\u0026#34;: amount} for item, amount in top_5_items] # Get the highest spending category highest_category = max(category_spending.items(), key=lambda x: x[1], default=(\u0026#34;None\u0026#34;, 0)) highest_spending_category = { \u0026#34;category\u0026#34;: highest_category[0], \u0026#34;amount\u0026#34;: highest_category[1] } # Get category breakdown for visualization category_breakdown = [{\u0026#34;category\u0026#34;: cat, \u0026#34;amount\u0026#34;: amount} for cat, amount in category_spending.items()] # Calculate projected end-of-month cost today = datetime.today() current_month = today.strftime(\u0026#34;%Y-%m\u0026#34;) days_passed = today.day days_in_month = (datetime(today.year, today.month % 12 + 1, 1) - datetime(today.year, today.month, 1)).days total_spent_current_month = monthly_spending.get(current_month, 0) projected_cost = (total_spent_current_month / days_passed) * days_in_month if days_passed \u0026gt; 0 else 0 # Calculate budget status budget_remaining = current_budget - total_spent_current_month budget_percentage = (total_spent_current_month / current_budget) * 100 if current_budget \u0026gt; 0 else 0 # Determine spending trend by comparing recent days # This would require more sophisticated analysis with time-series data # For now, we\u0026#39;ll return a simple placeholder spending_trend = \u0026#34;steady\u0026#34; # Options could be: \u0026#34;increasing\u0026#34;, \u0026#34;decreasing\u0026#34;, \u0026#34;steady\u0026#34; # Calculate spending velocity (average daily spend this month) daily_spending_rate = total_spent_current_month / days_passed if days_passed \u0026gt; 0 else 0 return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: { \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, # Enable CORS \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET, POST, PUT, DELETE, OPTIONS\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps({ \u0026#34;spending_data\u0026#34;: dict(spending_data), # Convert defaultdict to regular dict for JSON serialization \u0026#34;highest_spent_item\u0026#34;: highest_spent_item, \u0026#34;top_5_items\u0026#34;: top_5_items, \u0026#34;monthly_spending\u0026#34;: dict(monthly_spending), \u0026#34;projected_cost\u0026#34;: projected_cost, \u0026#34;current_budget\u0026#34;: current_budget, \u0026#34;budget_remaining\u0026#34;: budget_remaining, \u0026#34;budget_percentage\u0026#34;: budget_percentage, \u0026#34;highest_spending_category\u0026#34;: highest_spending_category, \u0026#34;category_breakdown\u0026#34;: category_breakdown, \u0026#34;spending_trend\u0026#34;: spending_trend, \u0026#34;daily_spending_rate\u0026#34;: daily_spending_rate, \u0026#34;total_spent\u0026#34;: total_spent }) } "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: Kick-off AWS FCJ Workforce - OJT FALL 2025\nDate \u0026amp; Time: 8:30, September 06, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/5.5-amplify/","title":"Deploy Frontend with Amplify","tags":[],"description":"","content":"In this section, you will deploy the Finance Tracker React application to AWS Amplify, which provides a complete hosting solution with CI/CD capabilities.\nOverview AWS Amplify will:\nHost your React frontend application Provide automatic builds on code commits Enable HTTPS by default Offer a custom domain (optional) Provide CI/CD pipeline from GitHub Prerequisites Before starting, ensure you have:\nA GitHub account The Finance Tracker repository forked to your account Repository URL: https://github.com/VDV314/financeTracker Steps to Deploy with Amplify 1. Navigate to AWS Amplify Console Open the AWS Management Console Search for AWS Amplify in the services search bar Click on AWS Amplify Click Create new app 2. Choose Source Code Provider On the Start building with Amplify page:\nUnder Deploy your app, select GitHub Click Next 3. Add Repository and Branch In the search box, enter: VDV314/financeTracker Select your repository from the dropdown For Branch, select main Click Next 4. Configure App Settings On the App settings page:\nApp name: Enter financeTracker Build settings: Amplify will auto-detect your framework The build settings should show:\nFramework: None (or React if detected) Frontend build command: (leave empty or auto-detected) Build output directory: / (root directory) 3. Click Next\n5. Review and Deploy Review all your settings: Repository details: github / VDV314/financeTracker Branch: main App settings: financeTracker Framework: None Advanced settings: Standard build, default image Click Save and deploy 7. financeTracker Overview In the Amplify Console overview, you can see:\nGet to production section, with options to:\nAdd a custom domain : Use your own domain with HTTPS\nEnable firewall protections : Use AWS WAF for enhanced security\nConnect new branches : Set up multiple environments\nBranches section displays:\n-The main branch (Production branch):\n-Domain : The Amplify-provided URL for your app\n-Last deployment : The timestamp of the latest deployment\n-Last commit : The latest Git commit message "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Workshop: Building Finance Tracker Application with AWS Serverless Overview In this workshop, you will learn how to build a complete serverless web application for personal finance management using AWS services. The Finance Tracker application allows users to track expenses, manage monthly budgets, and view financial statistics - all deployed on AWS serverless architecture.\nWorkshop Architecture The workshop uses a serverless architecture with the following main components:\nFrontend: React application hosted on AWS Amplify API Layer: Amazon API Gateway to expose REST API endpoints Business Logic: AWS Lambda function for business processing Data Storage: Amazon DynamoDB to store expense and budget data Security: AWS IAM to manage access permissions between services Workshop Objectives After completing this workshop, you will:\n‚úÖ Understand how to design and deploy serverless applications on AWS\n‚úÖ Create and configure DynamoDB tables for NoSQL data storage\n‚úÖ Write and deploy AWS Lambda functions using Python\n‚úÖ Create REST APIs with Amazon API Gateway\n‚úÖ Integrate API Gateway with Lambda functions\n‚úÖ Deploy React applications to AWS Amplify with CI/CD\n‚úÖ Configure IAM roles and policies for security\n‚úÖ Handle CORS for web applications\n‚úÖ Test and debug serverless applications\nContent Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 29/09/2025 to 22/11/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Finance Tracker Project and Workshop, through which I improved my skills in Programming, Gaining deeper understanding of AWS services.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"General Evaluation 1. Work Environment\nThe work environment is very friendly and open. FCJ team members are always ready to help when I encounter difficulties, even outside working hours. The workspace is neat and comfortable, helping me focus better. However, I think we could add more networking sessions or team bonding activities so everyone can understand each other better.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don\u0026rsquo;t understand, and always encourages me to ask questions. The team admin supports with procedures, documents, and creates favorable conditions for me to work smoothly. I highly appreciate that the mentor allows me to try and solve problems myself rather than just providing answers.\n3. Alignment Between Work and Major\nThe work assigned to me aligns with the knowledge I learned at school, while also expanding into new areas I had never encountered before. Thanks to this, I both reinforced my foundational knowledge and learned additional practical skills.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared many practical experiences that helped me better orient my career.\n5. Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still maintains a pleasant atmosphere. When there are urgent projects, everyone works together and supports each other regardless of position. This makes me feel like I\u0026rsquo;m part of the team, even though I\u0026rsquo;m just an intern.\n6. Policies / Benefits for Interns\nThe company provides internship allowances and offers flexible time arrangements when necessary. Additionally, being able to participate in internal training sessions is a big plus.\nAdditional Questions - What are you most satisfied with during your internship?\nWhat I am most satisfied with is the dedicated support from my mentor and team. My mentor not only provided detailed guidance but also encouraged me to think independently and solve problems, helping me develop independent thinking. Additionally, the friendly work environment and mutual support culture within the team made me feel welcomed and motivated to learn every day. Being able to participate in real projects and apply what I learned has been an extremely valuable experience.\n- What do you think the company should improve for future interns?\nI think the company could organize more team meetings to give interns opportunities to connect and understand each other better, especially when working remotely. Additionally, if possible, establishing a clearer roadmap for interns from the beginning - including goals to achieve and skills to develop - would help future interns orient themselves better and make the most of their internship time. Finally, enhancing regular feedback sessions (weekly or bi-weekly) would help interns know what they\u0026rsquo;re doing well and what areas need improvement.\n- If introducing to friends, would you recommend them to intern here? Why?\nYes, I would absolutely recommend friends to intern here. The main reasons are:\nHigh-quality learning environment: Working with experienced mentors, accessing technology and professional work processes Positive company culture: Everyone respects and supports each other, creating a comfortable atmosphere for interns Work aligned with major: Applying learned knowledge and developing practical skills Personal development opportunities: Not only learning technical skills but also practicing soft skills like teamwork, communication, and problem-solving This is an ideal environment for those who want to accumulate practical experience and prepare for their future careers.\nSuggestions \u0026amp; Expectations - Do you have any suggestions to improve the internship experience?\nI have several suggestions to improve the internship experience:\nOrganize more detailed orientation sessions: Could organize comprehensive introduction sessions about company culture, work processes, and tools used in the first week to help interns get accustomed faster.\nEstablish clear roadmap and objectives: Build a specific timeline for skills to be acquired and projects to be completed in each phase, helping interns self-assess their progress.\nEnhance regular feedback sessions: Could organize 1-on-1 meetings weekly or bi-weekly for mentors and interns to discuss progress, difficulties, and direction for upcoming work.\nOrganize more team meeting activities: Networking sessions, experience sharing among interns or with other teams to expand network and increase cohesion.\nProvide centralized learning materials: Create a shared repository containing tutorial documents, best practices, and case studies for interns to self-learn and reference.\n- Would you like to continue this program in the future?\nYes, I would very much like to continue this internship program or similar programs in the future.\nThe reasons are:\nProfessional learning environment: I feel I have developed greatly in both technical skills and soft skills in a short time. Excellent mentor and team: The dedicated support and professional working style of the team are things I want to continue learning from. Practical and meaningful work: Participating in real projects helped me understand the industry better and accumulate valuable experience. Suitable company culture: The open culture and mutual support create motivation for me to commit long-term. If given the opportunity, I hope to continue contributing and developing with the company, whether in part-time or full-time capacity.\n"},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://vdv314.github.io/Fcj-project-finance-personal/tags/","title":"Tags","tags":[],"description":"","content":""}]